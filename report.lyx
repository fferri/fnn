#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Word Prediction with Neural Nets
\end_layout

\begin_layout Author
Roberto Capobianco, Federico Ferri 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash

\backslash

\end_layout

\end_inset

 Sapienza University of Rome
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash

\backslash

\end_layout

\end_inset

 MSE in Artificial Intelligence and Robotics
\end_layout

\begin_layout Abstract
In order to improve speech recognizers accuracy, a word prediction method
 is proposed based on BP feedforward neural network.
 Firstly, the features are extracted from a text corpus, and then they are
 input into the BP neural network to train and obtain the optimal word predictor.
 The predictor is implemented as a N-gram classifier.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
The results show that the proposed method has improved the classification
 accuracy and reduced the training time compared with traditional classification
 methods.
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Table of contents
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Word prediction is a basic problem in speech recognition.
 It is not possible to perfectly identify phonemes in noisy speech because
 the acoustic input is often ambiguous: there are several different words
 that fit the acoustic signal equally well (wreck a nice beach; recognize
 speech).
\end_layout

\begin_layout Standard
People use their understanding of the meaning of the utterance to hear the
 right words.
 This is mostly done unconsciously, and the human brain is very good at
 it.
\end_layout

\begin_layout Standard
This means speech recognizers have to know which words are likely to come
 next and which are not.
 Fortunately, words can be predicted quite well without full understanding.
 The purpose of this project is, therefore, that of implementing a model
 of a neural network which performs this activity.
\end_layout

\begin_layout Section
Literature survey
\end_layout

\begin_layout Subsection
Standard Trigram method
\end_layout

\begin_layout Standard
The standard trigram method consists of taking a huge amount of text and
 counting the frequencies of all triples of words.
 Then using those frequencies to make bets on the relative probabilities
 of words, given the two previous words.
 Then we can compute the relative probability that the third word will be
 c (given first two words a, b), versus the third that would be d, is given
 by the ratios of the counts:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{p(w_{3}=c|w_{2}=b,w_{1}=a)}{p(w_{3}=d|w_{2}=b,w_{1}=a)}=\frac{count(abc)}{count(abd)}
\]

\end_inset


\end_layout

\begin_layout Standard
Until recently, this was the state of the art.
\end_layout

\begin_layout Standard
The problem is that we cannot use a bigger context, because there are too
 many possibilities to store and the counts would mostly be zero.
\end_layout

\begin_layout Standard
Moreover, the probability of an arbitrary sequence is not zero just because
 we haven't seen that example triple before!
\end_layout

\begin_layout Standard
Moreover, the trigram model does not understands similarity between words,
 for example consider the sentence 
\begin_inset Quotes eld
\end_inset

the cat got squashed in the garden on friday
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

the dog got flattened in the yard on monday
\begin_inset Quotes erd
\end_inset

; so it cannot use past experience to help with new similar -but previously
 never seen- sequences.
\end_layout

\begin_layout Subsection
Yoshua Bengio's Neural Probabilistic Model
\end_layout

\begin_layout Standard
To overcome these limitations, what we need to do is to convert words into
 a vector of semantic and syntactic features, and use the features of the
 previous words to predict the next word.
\end_layout

\begin_layout Standard
Using a feature representation also allows a context that contains many
 more previous words (e.g.
 10).
\end_layout

\begin_layout Standard
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fig/bengio1.eps
	width 11cm

\end_inset


\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
The main problem with this architecture is that, in the output, we have
 a very high (i.e.
 100000) number of outgoing connections, but we cannot afford to have a
 very high number of hidden units.
\end_layout

\begin_layout Subsection
A Serial Architecture
\end_layout

\begin_layout Standard
One way to avoid the problem is to slightly modify the structure as follows:
\end_layout

\begin_layout Standard
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fig/bengio2.eps
	width 11cm

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
In this architecture we put in the input layers, not only the previous words,
 but also the candidate next word, and what we output is a score for how
 good is that word in that context.
\end_layout

\begin_layout Standard
(we have to run forward this net many many times)
\end_layout

\begin_layout Standard
So we try all the candidate words one at a time.
\end_layout

\begin_layout Standard
This allows the learned feature vector representation to be used for the
 candidate word.
\end_layout

\begin_layout Standard
After computing the logit score for each candidate word, we use all of the
 logits in a softmax to get word probabilities.
\end_layout

\begin_layout Section
Neural network design
\end_layout

\begin_layout Subsection
FANN Library
\end_layout

\begin_layout Standard
The implementation of this project has been realized through the use of
 the Fast Artificial Neural Network (FANN) Library, which is a free open
 source neural network library.
 It implements multilayer artificial neural networks and it is realized
 in the C programming language, also including the support for both fully
 connected and sparsely connected networks.
 This library includes a framework for the easy handling of training datasets,
 easy saving and loading of entire Artificial Neural Networks.
 The team decided to use FANN library, among many other libraries, for several
 reasons: it is fast, easy to implement, open-source (allowing us to alter
 it if we needed to) and it is written in C (having no problems for the
 integration with the C++ programming language).
 Furthermore, FANN implements a structure where the connections are the
 central structure, consisting of three levels:
\end_layout

\begin_layout Enumerate
fann: the ANN with references to the connection layers.
 
\end_layout

\begin_layout Enumerate
fann_connection_layer: the connection layers with references to the connections.
 
\end_layout

\begin_layout Enumerate
fann_connection: the connections with a weight and two pointers to the two
 connected neurons.
\end_layout

\begin_layout Standard
The main advantage of this structure is that one simple loop can run through
 all the connections between two layers.
 If these connections are allocated in one long array, the array can be
 processed completely sequentially.
 This architecture is shown in the following scheme:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fig/fann.eps
	scale 40

\end_inset


\end_layout

\begin_layout Subsection
Network Architecture
\end_layout

\begin_layout Standard
In order to solve the problem, a feedforward artificial neural network has
 been used.
\end_layout

\begin_layout Standard
The network is composed by an input layer, whose size is N times the number
 of words in the dictionary; two hidden layers; and an output layer.
\end_layout

\begin_layout Standard
All the units are logistic units with a sigmoid activation function 
\begin_inset Formula $\frac{1}{1+e^{-kx}}$
\end_inset

.
\end_layout

\begin_layout Standard
The output layer it is just one unit, and represents the logit of that word
 N-gram.
 A high output (
\begin_inset Formula $\thickapprox1$
\end_inset

) means that a N-gram is very likely, while a low output (
\begin_inset Formula $\thickapprox-1$
\end_inset

) means that a N-gram is very unlikely.
\end_layout

\begin_layout Standard
In the prediction task, the output is computed for every candidate word,
 and all the logits are transformed according to a softmax activation function:
\begin_inset Formula 
\[
f(x_{i})=\frac{e^{x_{i}}}{\sum_{j}e^{x_{j}}}
\]

\end_inset


\end_layout

\begin_layout Standard
In this way, we get a probability distribution of all the candidate words.
\end_layout

\begin_layout Standard
As a detail of the implementation of this project, the cost function that
 has been used is the standard Mean Square Error function of the FANN library.
\end_layout

\begin_layout Subsection
Training Algorithm
\end_layout

\begin_layout Standard
The training algorithm that has been used in the project is RPROP (short
 for resilient backpropagation), which is a learning heuristic for supervised
 learning in feedforward artificial neural networks.
 We have chosen to use this algorithm because, next to the cascade correlation
 algorithm and the Levenberg–Marquardt algorithm, RPROP is one of the fastest
 weight update mechanisms.
 Furthermore, RPROP is a batch update algorithm, thus being more suitable
 for this kind of problems having big datasets.
 In detail, RPROP takes into account only the sign of the partial derivative
 over all patterns (not the magnitude), and acts independently on each "weight".
 For each weight w
\begin_inset script superscript

\begin_layout Plain Layout
i
\end_layout

\end_inset

, if there was a sign change of the partial derivative (
\begin_inset Formula $\frac{\partial E(t)}{\partial w^{i}}$
\end_inset

) of the total error function compared to the last iteration, the update
 value 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\Delta^{i}(t)$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 for that weight is multiplied by a factor 
\begin_inset Formula $\eta^{-}$
\end_inset

, where 
\begin_inset Formula $\eta^{-}<1$
\end_inset

.
 If the last iteration t produced the same sign, the update value is multiplied
 by a factor of 
\begin_inset Formula $\eta^{+}$
\end_inset

, where 
\begin_inset Formula $\eta^{+}>1$
\end_inset

.
 Finally the weights are update as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
w^{i}\left(t+1\right)=w^{i}\left(t\right)-sign\left(\frac{\partial E(t)}{\partial w^{i}}\right)\cdot\Delta^{i}\left(t\right)
\]

\end_inset


\end_layout

\begin_layout Subsection
Cost function
\end_layout

\begin_layout Standard
Two different cost functions have been implemented and tested.
\end_layout

\begin_layout Standard
Mean Square Error cost function is defined as: 
\begin_inset Formula 
\[
E(w)=\frac{1}{2N}\sum_{i=1}^{N}\left(t_{i}-y_{i}\right)^{2}
\]

\end_inset

This cost function has been used for training without having the softmax
 layer during training (the softmax is computed later, in the prediction
 phase).
\end_layout

\begin_layout Standard
Cross-Entropy cost function is defined as: 
\begin_inset Formula 
\[
E(w)=-\frac{1}{N}\sum_{i=1}^{N}t_{i}log\left(y_{i}\right)-\left(1-t_{i}\right)log\left(1-y_{i}\right)
\]

\end_inset

This cost function has been used for training with a softmax output layer.
 In this case, the output layer has been changed to have 2 units -one for
 each class-, in order to make the softmax activation function work properly.
\end_layout

\begin_layout Standard
Cross-Entropy cost function is more suitable for classifiers.
 The cost is zero when 
\begin_inset Formula $t_{i}$
\end_inset

 is equal to 
\begin_inset Formula $y_{i}$
\end_inset

 (the instance is classified correctly), and it is very big (
\begin_inset Formula $\thickapprox\infty$
\end_inset

) when the instance is classified incorrectly (i.e.: 
\begin_inset Formula $t_{i}=1-y_{i}$
\end_inset

 or vice versa).
\end_layout

\begin_layout Standard
This greatly speeds up the convergence of the minimization algorithm.
\end_layout

\begin_layout Section
Dataset
\end_layout

\begin_layout Standard
One of the most challenging problems of this project was that of choosing
 a suitable dataset in order to obtain a good word prediction.
 The size of the dataset used for the training and validation procedure,
 in fact, is a highly influencing aspect of the work.
\end_layout

\begin_layout Subsection
Dataset source
\end_layout

\begin_layout Standard
In order to obtain and produce a good quantity of text, suitable for the
 neural network training, validation and test, the team downloaded and pre
 processed many free e-books provided by the 
\begin_inset Quotes eld
\end_inset

Project Gutenberg
\begin_inset Quotes erd
\end_inset

.
 In detail, we produced two different datasets, which have been split in
 three parts, respectively devoted to the training, the validation and the
 test procedures.
\end_layout

\begin_layout Subsection
Dataset encoding and dictionary size
\end_layout

\begin_layout Standard
Due to the structure of the network which has been chosen for the implementation
 of this project, with one unit for each possible nominal value of the input
 words, there was the need of representing each of them using an input vector.
 In detail, the vector has been produced using a 1-of-N encoding, with one
 bit for each possible word in the dictionary.
 In this way, during the training procedure, only one of the units is active
 and all the others are off.
 
\end_layout

\begin_layout Standard
Therefore, the first step of our pre processing work has been that of producing
 a dictionary containing all the words appearing in the dataset.
 However, taking into account also the words appearing few times in the
 dataset led to very big input vectors, increasing the time and the computationa
l resources required by the training procedure.
 Even if we don't know if the words having a low presence in the text would
 have a positive or negative influence in the learning phase, the team decided
 for a trade-off between the dictionary (and consequently the inputs) size,
 the need for a quite big and various dataset and the low computational
 capabilities available.
 In order to achieve the objective of reducing the dictionary size, mantaining
 a various dataset, we produced a statistics file containing a number for
 each word corresponding to the times that it appeared in the text.
 Through the use of these statistics, a new dictionary containing only the
 words appearing more times than an assigned threshold has been produced.
 In this way, all the phrases in the raw dataset containing words which
 were not in the final dictionary have been no more considered in the final
 dataset.
 As a main result of this procedure, we obtained a big reduction of the
 input vector.
\end_layout

\begin_layout Subsection
N-gram model
\end_layout

\begin_layout Standard
The sequences of words which are provided as an input for the neural network
 are modeled using a n-gram distribution.
 In detail, words are modeled such that each n-gram is composed of n words.
 Obviously, the only explicit dependency range is (n-1) tokens for an n-gram
 model, and since natural languages incorporate many cases of unbounded
 dependencies, this means that an n-gram model cannot in principle distinguish
 unbounded dependencies from noise.
 In our experiments, we mainly used trigrams that have been generated from
 the dataset obtained with the procedure described in the previous section.
 During this sequencing activity, we also pre processed strings in order
 to remove strange characters, to collapse subsequent whitespaces to a single
 space and reducing punctuation.
 Finally, we applied the 1-of-N encoding to each n-gram, thus obtaining
 a (N*n) input vector, which also determined the size of the network.
\end_layout

\begin_layout Subsection
Negative examples generation
\end_layout

\begin_layout Standard
In order to provide a good dataset as an input for the neural network, we
 also had to introduce negative classification examples.
 These inputs, consisting of meaningless trigrams having a -1 score, have
 been generated through a random composition of words from the dictionary.
 Though not fully guaranteeing the generation of meaningless examples, this
 approach has been useful in order to automatically obtain the desired 
\begin_inset Quotes eld
\end_inset

wrong
\begin_inset Quotes erd
\end_inset

 data from the same dictionary of the meaningful trigrams, also achieving
 some kind of robustness to noise.
\end_layout

\begin_layout Subsection
Training & Regularization
\end_layout

\begin_layout Standard
As already stated, the training activity has been performed over the training
 dataset, which roughly represents the 60% of the whole corpus.
 Furthermore, the validation set, consisting of 20% of the corpus, has been
 used during the whole training time in order to test the classification
 error and to prevent the overfitting problem, according to a technique
 which is called early stopping.
\end_layout

\begin_layout Subsection
Results
\end_layout

\begin_layout Standard
We found the optimal meta-parameters for the network by performing the model
 validation, against a separate validation dataset.
 The meta-parameters that we have been looking for regard only the size
 of the network (the number of units in the two hidden layers).
\end_layout

\begin_layout Standard
In our experiments, the model that scored better over the validation set,
 had 200 + 60 hidden units, and achieved 6.77% error on the validation set,
 and 7.71% error on the test set.
\end_layout

\begin_layout Standard
Bigger models have been trained, but not proven useful.
 In fact they need more time to train, and do overfit the data more easily.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fig/plot-h100-H120-MSE.eps
	width 11cm

\end_inset


\end_layout

\begin_layout Standard
Plot of MSE for training with 100 hidden units and 120 hidden units, for
 40 epochs.
 Blue track: training error, Green track: validation error.
 Early stopping at 17th epoch.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fig/plot-h200-H60-MSE.eps
	width 11cm

\end_inset


\end_layout

\begin_layout Standard
Plot of MSE for training with 200 hidden units and 60 hidden units, for
 20 epochs.
 Blue track: training error, Green track: validation error.
 Early stopping at 17th epoch.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fig/plot-h1000-H600-MSE.eps
	width 11cm

\end_inset


\end_layout

\begin_layout Standard
Plot of MSE for training with 1000 hidden units and 600 hidden units, for
 14 epochs.
 Blue track: training error, Green track: validation error.
 Early stopping at 10th epoch.
\end_layout

\begin_layout Subsection
Conclusions and Further Work
\end_layout

\begin_layout Standard
As expected, the Cross-Entropy cost function in conjunction with the softmax
 activation function in the output layer gave better results than using
 a MSE cost function with a sigmoid unit in the output layer: the training
 has been faster, and the accuracy greater.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "REFERENCES"
options "bibtotoc,plain"

\end_inset


\end_layout

\end_body
\end_document
