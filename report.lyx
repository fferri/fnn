#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Word prediction with Neural Nets
\end_layout

\begin_layout Abstract
Abstract goes here
\end_layout

\begin_layout Section*
Table of contents
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
blah +++
\end_layout

\begin_layout Section*
Literature survey
\end_layout

\begin_layout Standard
Standard trigram method consists of taking a huge amount of text and counting
 the frequencies of all triples of words.
 Then using those frequencies to make bets on the relative probabilities
 of words, given the two previous words.
 Then we can compute the relative probability that the third word will be
 c (given first two words a, b), versus the thir that would be d, is given
 by the ratios of the counts:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{p(w_{3}=c|w_{2}=b,w_{1}=a)}{p(w_{3}=d|w_{2}=b,w_{1}=a)}=\frac{count(abc)}{count(abd)}
\]

\end_inset


\end_layout

\begin_layout Standard
Until recently, this was the state of the art.
\end_layout

\begin_layout Standard
The problem is that we cannot use a bigger context, because there are too
 many possibilities to store and the counts would mostly be zero.
\end_layout

\begin_layout Standard
Moreover, the probability of an arbitrary sequence is not zero just because
 we haven't seen that example triple before!
\end_layout

\begin_layout Standard
Moreover, the trigram model does not understands similarity between words,
 for example consider the sentence 
\begin_inset Quotes eld
\end_inset

the cat got squashed in the garden on friday
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

the dog got flattened in the yard on monday
\begin_inset Quotes erd
\end_inset

; so it cannot use past experience to help with new similar -but previously
 never seen- sequences.
\end_layout

\begin_layout Standard
To overcome this limitation, what we need to do is convert words into a
 vector of semantic and syntactic features, and use the features of the
 previous words to predict the next word.
\end_layout

\begin_layout Standard
Using a feature representation also allows a context that contains many
 more previous words (e.g.
 10).
\end_layout

\begin_layout Standard
<<inserire diagramma 4-4 @4:35 Bengio's>>
\end_layout

\begin_layout Standard
The problem with this architecture is that in the output, we have a very
 high (i.e.
 100000) number of outgoing connections.
\end_layout

\begin_layout Standard
One way to avoid is to slightly modify the structure as follows:
\end_layout

\begin_layout Standard
<<inserire diagramma 4-5 @1:10>>
\end_layout

\begin_layout Standard
where we put in the input layers, not only the previous words, but also
 the candidate next word, and what we output is a score for how good is
 that word in that context.
\end_layout

\begin_layout Standard
(we have to run forward this net many many times)
\end_layout

\begin_layout Standard
So we try all the candidate words one at a time.
\end_layout

\begin_layout Standard
This allows the learned feature vector representation to be used for the
 candidate word.
\end_layout

\begin_layout Standard
After computing the logit score for each candidate word, use all of the
 logits in a softmax to get word probabilities
\end_layout

\begin_layout Part*
Neural network design
\end_layout

\begin_layout Section*
Pre & Post Processing
\end_layout

\begin_layout Standard
blah blah about dataset this is a trial
\end_layout

\begin_layout Section*
Neural network structure
\end_layout

\begin_layout Standard
repeat blah blah about structure
\end_layout

\begin_layout Section*
Training Algorithm
\end_layout

\begin_layout Standard
blah!
\end_layout

\begin_layout Section*
Test Procedure
\end_layout

\begin_layout Subsection*
Bias and variance issues
\end_layout

\begin_layout Standard
blah blah overfitting
\end_layout

\begin_layout Section*
Results
\end_layout

\begin_layout Part*
Conclusions and Further Work
\end_layout

\begin_layout Standard
blah blah about possible RNN
\end_layout

\begin_layout Part*
References
\end_layout

\end_body
\end_document
