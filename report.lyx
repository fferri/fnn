#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Word prediction with Neural Nets
\end_layout

\begin_layout Abstract
Abstract goes here
\end_layout

\begin_layout Abstract
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Table of contents
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Word prediction is a basic problem in speech recognition.
 It is not possible to perfectly identify phonemes in noisy speech; the
 acoustic input is often ambiguous: there are several different words that
 fit the acoustic signal equally well (wreck a nice beach; recognize speech).
\end_layout

\begin_layout Standard
People use their understanding of the meaning of the utterance to hear the
 right words.
 This is mostly done unconsciously, and the human brain is very good at
 it.
\end_layout

\begin_layout Standard
This means speech recognizers have to know which words are likely to come
 next and which are not.
 Fortunately, words can be predicted quite well without full understanding.
\end_layout

\begin_layout Section
Literature survey
\end_layout

\begin_layout Subsection
Standard Trigram method
\end_layout

\begin_layout Standard
Standard trigram method consists of taking a huge amount of text and counting
 the frequencies of all triples of words.
 Then using those frequencies to make bets on the relative probabilities
 of words, given the two previous words.
 Then we can compute the relative probability that the third word will be
 c (given first two words a, b), versus the third that would be d, is given
 by the ratios of the counts:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{p(w_{3}=c|w_{2}=b,w_{1}=a)}{p(w_{3}=d|w_{2}=b,w_{1}=a)}=\frac{count(abc)}{count(abd)}
\]

\end_inset


\end_layout

\begin_layout Standard
Until recently, this was the state of the art.
\end_layout

\begin_layout Standard
The problem is that we cannot use a bigger context, because there are too
 many possibilities to store and the counts would mostly be zero.
\end_layout

\begin_layout Standard
Moreover, the probability of an arbitrary sequence is not zero just because
 we haven't seen that example triple before!
\end_layout

\begin_layout Standard
Moreover, the trigram model does not understands similarity between words,
 for example consider the sentence 
\begin_inset Quotes eld
\end_inset

the cat got squashed in the garden on friday
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

the dog got flattened in the yard on monday
\begin_inset Quotes erd
\end_inset

; so it cannot use past experience to help with new similar -but previously
 never seen- sequences.
\end_layout

\begin_layout Subsection
Yoshua Bengio's Neural Probabilistic Model
\end_layout

\begin_layout Standard
To overcome this limitation, what we need to do is convert words into a
 vector of semantic and syntactic features, and use the features of the
 previous words to predict the next word.
\end_layout

\begin_layout Standard
Using a feature representation also allows a context that contains many
 more previous words (e.g.
 10).
\end_layout

\begin_layout Standard
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fig/bengio1.eps
	width 11cm

\end_inset


\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
The problem with this architecture is that in the output, we have a very
 high (i.e.
 100000) number of outgoing connections, but we cannot afford to have a
 very high number of hidden units.
\end_layout

\begin_layout Subsection
A Serial Architecture
\end_layout

\begin_layout Standard
One way to avoid is to slightly modify the structure as follows:
\end_layout

\begin_layout Standard
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fig/bengio2.eps
	width 11cm

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
where we put in the input layers, not only the previous words, but also
 the candidate next word, and what we output is a score for how good is
 that word in that context.
\end_layout

\begin_layout Standard
(we have to run forward this net many many times)
\end_layout

\begin_layout Standard
So we try all the candidate words one at a time.
\end_layout

\begin_layout Standard
This allows the learned feature vector representation to be used for the
 candidate word.
\end_layout

\begin_layout Standard
After computing the logit score for each candidate word, use all of the
 logits in a softmax to get word probabilities
\end_layout

\begin_layout Section
Neural network design
\end_layout

\begin_layout Subsection
Network Architecture
\end_layout

\begin_layout Standard
For solving the problem, a feedforward artificial neural network has been
 used.
\end_layout

\begin_layout Standard
The network is composed by an input layer, whose size is N times the number
 of words in the dictionary; two hidden layers; and an output layer.
\end_layout

\begin_layout Standard
All units are logistic units with a sigmoid activation function 
\begin_inset Formula $\frac{1}{1+e^{-kx}}$
\end_inset

.
\end_layout

\begin_layout Standard
The output layer it is just one unit, and represent the logit of that word
 N-gram.
 High output (
\begin_inset Formula $\thickapprox1$
\end_inset

) means a N-gram is very likely.
 Low output (
\begin_inset Formula $\thickapprox-1$
\end_inset

) means a N-gram is very unlikely.
\end_layout

\begin_layout Standard
In the prediction task, the output is computed for every candidate word,
 and all the logits are transformed according to a softmax activation function:
\begin_inset Formula 
\[
f(x_{i})=\frac{e^{x_{i}}}{\sum_{j}e^{x_{j}}}
\]

\end_inset


\end_layout

\begin_layout Standard
so that we get a probability distribution of all the candidate words.
\end_layout

\begin_layout Standard
The cost function that has been used is the standard Mean Square Error function
 of FANN.
\end_layout

\begin_layout Subsection
Training Algorithm
\end_layout

\begin_layout Standard
The training algorithm that has been used is RPROP (short for resilient
 backpropagation), which is a learning heuristic for supervised learning
 in feedforward artificial neural networks.
 Next to the cascade correlation algorithm and the Levenbergâ€“Marquardt algorithm
, RPROP is one of the fastest weight update mechanisms.
 RPROP is a batch update algorithm.
\end_layout

\begin_layout Section
Dataset
\end_layout

\begin_layout Standard
One of the most challenging problems of this project was that of choosing
 a suitable dataset in order to obtain a good prediction.
 The size of the dataset used for the training and validation procedure,
 in fact, is a highly influencing aspect of this work.
\end_layout

\begin_layout Subsection
Dataset source
\end_layout

\begin_layout Standard
In order to obtain and produce a good quantity of text, suitable for the
 neural network training, validation and test, the team downloaded and pre
 processed many free e-books provided by the 
\begin_inset Quotes eld
\end_inset

Project Gutenberg
\begin_inset Quotes erd
\end_inset

.
 In detail, we produced two different datasets, which have been split in
 three parts, respectively devoted to the training, the validation and the
 test procedures.
\end_layout

\begin_layout Subsection
Dataset encoding and dictionary size
\end_layout

\begin_layout Standard
Due to the structure of the network which has been chosen for the implementation
 of this project, with one unit for each possible nominal value of the input
 words, there was the need of representing each of them using an input vector.
 In detail, the vector has been produced using a 1-of-N encoding, with one
 bit for each possible word in the dictionary.
 In this way, during the training procedure, only one of the units is active
 and all the others are off.
 
\end_layout

\begin_layout Standard
Therefore, the first step of our pre processing work has been that of producing
 a dictionary containing all the words appearing in the dataset.
 However, taking into account also the words appearing few times in the
 dataset led to very big input vectors, increasing the time and the computationa
l resources required by the training procedure.
 Even if we don't know if the words having a low presence in the text would
 have a positive or negative influence in the learning phase, the team decided
 for a trade-off between the dictionary (and consequently the inputs) size,
 the need for a quite big and various dataset and the low computational
 capabilities available.
 In order to achieve the objective of reducing the dictionary size, mantaining
 a various dataset, we produced a statistics file containing a number for
 each word corresponding to the times that it appeared in the text.
 Basing on these statistics file a new dictionary containing only the words
 appearing more times than an assigned treshold has been produced.
 In this way, all the phrases in the raw dataset containing words not present
 in the dictionary itself have not been considered in the final dataset.
\end_layout

\begin_layout Subsection
N-gram model
\end_layout

\begin_layout Standard
*Negative examples generation
\end_layout

\begin_layout Section
Training & Regularization
\end_layout

\begin_layout Standard
Training has been performed over the training dataset, which consists of
 roughly 60% of the whole corpus.
 Validation set, consisting of 20% of the corpus, has been used all time
 during the training to be tested for classifaction error, to prevent overfittin
g.
 This technique is called early stopping.
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig/plot-h100-H120-MSE.eps
	width 12cm

\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout
Plot of MSE for training with 100 hidden units and 120 hidden units, for
 40 epochs.
 Blue track: training error, Green track: validation error.
 Early stopping at 17th epoch.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
Plot of MSE for training with 200 hidden units and 60 hidden units, for
 20 epochs.
 Blue track: training error, Green track: validation error.
 Early stopping at 17th epoch.
\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig/plot-h200-H60-MSE.eps

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
Plot of MSE for training with 1000 hidden units and 600 hidden units, for
 14 epochs.
 Blue track: training error, Green track: validation error.
 Early stopping at 10th epoch.
\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig/plot-h1000-H600-MSE.eps

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusions and Further Work
\end_layout

\begin_layout Standard
blah blah about possible RNN
\end_layout

\begin_layout Part*
References
\end_layout

\end_body
\end_document
