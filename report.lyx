#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Word Prediction with Neural Nets
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Author
Roberto Capobianco, Federico Ferri 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash

\backslash

\end_layout

\end_inset

 Sapienza University of Rome
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash

\backslash

\end_layout

\end_inset

 MSE in Artificial Intelligence and Robotics
\end_layout

\begin_layout Abstract
Abstract goes here
\end_layout

\begin_layout Abstract
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Table of contents
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Word prediction is a basic problem in speech recognition.
 It is not possible to perfectly identify phonemes in noisy speech because
 the acoustic input is often ambiguous: there are several different words
 that fit the acoustic signal equally well (wreck a nice beach; recognize
 speech).
\end_layout

\begin_layout Standard
People use their understanding of the meaning of the utterance to hear the
 right words.
 This is mostly done unconsciously, and the human brain is very good at
 it.
\end_layout

\begin_layout Standard
This means speech recognizers have to know which words are likely to come
 next and which are not.
 Fortunately, words can be predicted quite well without full understanding.
 The purpose of this project is, therefore, that of implementing a model
 of a neural network which performs this activity.
\end_layout

\begin_layout Section
Literature survey
\end_layout

\begin_layout Subsection
Standard Trigram method
\end_layout

\begin_layout Standard
The standard trigram method consists of taking a huge amount of text and
 counting the frequencies of all triples of words.
 Then using those frequencies to make bets on the relative probabilities
 of words, given the two previous words.
 Then we can compute the relative probability that the third word will be
 c (given first two words a, b), versus the third that would be d, is given
 by the ratios of the counts:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{p(w_{3}=c|w_{2}=b,w_{1}=a)}{p(w_{3}=d|w_{2}=b,w_{1}=a)}=\frac{count(abc)}{count(abd)}
\]

\end_inset


\end_layout

\begin_layout Standard
Until recently, this was the state of the art.
\end_layout

\begin_layout Standard
The problem is that we cannot use a bigger context, because there are too
 many possibilities to store and the counts would mostly be zero.
\end_layout

\begin_layout Standard
Moreover, the probability of an arbitrary sequence is not zero just because
 we haven't seen that example triple before!
\end_layout

\begin_layout Standard
Moreover, the trigram model does not understands similarity between words,
 for example consider the sentence 
\begin_inset Quotes eld
\end_inset

the cat got squashed in the garden on friday
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

the dog got flattened in the yard on monday
\begin_inset Quotes erd
\end_inset

; so it cannot use past experience to help with new similar -but previously
 never seen- sequences.
\end_layout

\begin_layout Subsection
Yoshua Bengio's Neural Probabilistic Model
\end_layout

\begin_layout Standard
To overcome these limitations, what we need to do is to convert words into
 a vector of semantic and syntactic features, and use the features of the
 previous words to predict the next word.
\end_layout

\begin_layout Standard
Using a feature representation also allows a context that contains many
 more previous words (e.g.
 10).
\end_layout

\begin_layout Standard
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fig/bengio1.eps
	width 11cm

\end_inset


\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
The main problem with this architecture is that, in the output, we have
 a very high (i.e.
 100000) number of outgoing connections, but we cannot afford to have a
 very high number of hidden units.
\end_layout

\begin_layout Subsection
A Serial Architecture
\end_layout

\begin_layout Standard
One way to avoid the problem is to slightly modify the structure as follows:
\end_layout

\begin_layout Standard
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fig/bengio2.eps
	width 11cm

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
In this architecture we put in the input layers, not only the previous words,
 but also the candidate next word, and what we output is a score for how
 good is that word in that context.
\end_layout

\begin_layout Standard
(we have to run forward this net many many times)
\end_layout

\begin_layout Standard
So we try all the candidate words one at a time.
\end_layout

\begin_layout Standard
This allows the learned feature vector representation to be used for the
 candidate word.
\end_layout

\begin_layout Standard
After computing the logit score for each candidate word, we use all of the
 logits in a softmax to get word probabilities.
\end_layout

\begin_layout Section
Neural network design
\end_layout

\begin_layout Subsection
Network Architecture
\end_layout

\begin_layout Standard
In order to solve the problem, a feedforward artificial neural network has
 been used.
\end_layout

\begin_layout Standard
The network is composed by an input layer, whose size is N times the number
 of words in the dictionary; two hidden layers; and an output layer.
\end_layout

\begin_layout Standard
All the units are logistic units with a sigmoid activation function 
\begin_inset Formula $\frac{1}{1+e^{-kx}}$
\end_inset

.
\end_layout

\begin_layout Standard
The output layer it is just one unit, and represents the logit of that word
 N-gram.
 A high output (
\begin_inset Formula $\thickapprox1$
\end_inset

) means that a N-gram is very likely, while a low output (
\begin_inset Formula $\thickapprox-1$
\end_inset

) means that a N-gram is very unlikely.
\end_layout

\begin_layout Standard
In the prediction task, the output is computed for every candidate word,
 and all the logits are transformed according to a softmax activation function:
\begin_inset Formula 
\[
f(x_{i})=\frac{e^{x_{i}}}{\sum_{j}e^{x_{j}}}
\]

\end_inset


\end_layout

\begin_layout Standard
In this way, we get a probability distribution of all the candidate words.
\end_layout

\begin_layout Standard
As a detail of the implementation of this project, the cost function that
 has been used is the standard Mean Square Error function of the FANN library.
\end_layout

\begin_layout Subsection
FANN Library
\end_layout

\begin_layout Standard
some notion about FANN
\end_layout

\begin_layout Subsection
Training Algorithm
\end_layout

\begin_layout Standard
The training algorithm that has been used in the project is RPROP (short
 for resilient backpropagation), which is a learning heuristic for supervised
 learning in feedforward artificial neural networks.
 We have chosen to use this algorithm because, next to the cascade correlation
 algorithm and the Levenbergâ€“Marquardt algorithm, RPROP is one of the fastest
 weight update mechanisms.
 Furthermore, RPROP is a batch update algorithm, thus being more suitable
 for this kind of problems having big datasets.
\end_layout

\begin_layout Section
Dataset
\end_layout

\begin_layout Standard
One of the most challenging problems of this project was that of choosing
 a suitable dataset in order to obtain a good word prediction.
 The size of the dataset used for the training and validation procedure,
 in fact, is a highly influencing aspect of the work.
\end_layout

\begin_layout Subsection
Dataset source
\end_layout

\begin_layout Standard
In order to obtain and produce a good quantity of text, suitable for the
 neural network training, validation and test, the team downloaded and pre
 processed many free e-books provided by the 
\begin_inset Quotes eld
\end_inset

Project Gutenberg
\begin_inset Quotes erd
\end_inset

.
 In detail, we produced two different datasets, which have been split in
 three parts, respectively devoted to the training, the validation and the
 test procedures.
\end_layout

\begin_layout Subsection
Dataset encoding and dictionary size
\end_layout

\begin_layout Standard
Due to the structure of the network which has been chosen for the implementation
 of this project, with one unit for each possible nominal value of the input
 words, there was the need of representing each of them using an input vector.
 In detail, the vector has been produced using a 1-of-N encoding, with one
 bit for each possible word in the dictionary.
 In this way, during the training procedure, only one of the units is active
 and all the others are off.
 
\end_layout

\begin_layout Standard
Therefore, the first step of our pre processing work has been that of producing
 a dictionary containing all the words appearing in the dataset.
 However, taking into account also the words appearing few times in the
 dataset led to very big input vectors, increasing the time and the computationa
l resources required by the training procedure.
 Even if we don't know if the words having a low presence in the text would
 have a positive or negative influence in the learning phase, the team decided
 for a trade-off between the dictionary (and consequently the inputs) size,
 the need for a quite big and various dataset and the low computational
 capabilities available.
 In order to achieve the objective of reducing the dictionary size, mantaining
 a various dataset, we produced a statistics file containing a number for
 each word corresponding to the times that it appeared in the text.
 Through the use of these statistics, a new dictionary containing only the
 words appearing more times than an assigned treshold has been produced.
 In this way, all the phrases in the raw dataset containing words which
 were not in the final dictionary have been no more considered in the final
 dataset.
 As a main result of this procedure, we obtained a big reduction of the
 input vector.
\end_layout

\begin_layout Subsection
n-gram model
\end_layout

\begin_layout Standard
The sequences of words which are provided as an input for the neural network
 are modeled using a n-gram distribution.
 In detail, words are modeled such that each n-gram is composed of n words.
 Obviously, the only explicit dependency range is (n-1) tokens for an n-gram
 model, and since natural languages incorporate many cases of unbounded
 dependencies, this means that an n-gram model cannot in principle distinguish
 unbounded dependencies from noise.
 In our experiments, we mainly used trigrams that have been generated from
 the dataset obtained with the procedure described in the previous section.
 During this sequencing activity, we also pre processed strings in order
 to remove strange characters, to collapse subsequent whitespaces to a single
 space and reducing punctuation.
 Finally, we applied the 1-of-N encoding to each n-gram, thus obtaining
 a (N*n) input vector, which also determined the size of the network.
\end_layout

\begin_layout Subsection
Negative examples generation
\end_layout

\begin_layout Section
Training & Regularization
\end_layout

\begin_layout Standard
Training has been performed over the training dataset, which consists of
 roughly 60% of the whole corpus.
 Validation set, consisting of 20% of the corpus, has been used all time
 during the training to be tested for classifaction error, to prevent overfittin
g.
 This technique is called early stopping.
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
We found optimal meta-parameters for the network by performing model validation,
 against a separate validation dataset.
 The meta-parameters that have been looked for are only the size of the
 network (the number of units in the two hidden layers).
\end_layout

\begin_layout Standard
The model that scored better over the validation set, had 200 + 60 hidden
 units, and achieved 6.77% error on the validation set, and 7.71% error on
 the test set.
\end_layout

\begin_layout Standard
Bigger models have been trained, but not proven useful.
 In fact they need more time to train, and do overfit the data more easily.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fig/plot-h100-H120-MSE.eps
	width 11cm

\end_inset


\end_layout

\begin_layout Standard
Plot of MSE for training with 100 hidden units and 120 hidden units, for
 40 epochs.
 Blue track: training error, Green track: validation error.
 Early stopping at 17th epoch.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fig/plot-h200-H60-MSE.eps
	width 11cm

\end_inset


\end_layout

\begin_layout Standard
Plot of MSE for training with 200 hidden units and 60 hidden units, for
 20 epochs.
 Blue track: training error, Green track: validation error.
 Early stopping at 17th epoch.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fig/plot-h1000-H600-MSE.eps
	width 11cm

\end_inset


\end_layout

\begin_layout Standard
Plot of MSE for training with 1000 hidden units and 600 hidden units, for
 14 epochs.
 Blue track: training error, Green track: validation error.
 Early stopping at 10th epoch.
\end_layout

\begin_layout Section
Conclusions and Further Work
\end_layout

\begin_layout Standard
blah blah about possible RNN
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "REFERENCES"
options "bibtotoc,plain"

\end_inset


\end_layout

\end_body
\end_document
