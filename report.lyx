#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Word prediction with Neural Nets
\end_layout

\begin_layout Abstract
Abstract goes here
\end_layout

\begin_layout Section*
Table of contents
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
Word prediction is a basic problem in speech recognition.
 It is not possible to perfectly identify phonemes in noisy speech; the
 acoustic input is often ambiguous: there are several different words that
 fit the acoustic signal equally well (wreck a nice beach; recognize speech).
\end_layout

\begin_layout Standard
People use their understanding of the meaning of the utterance to hear the
 right words.
 This is mostly done unconsciously, and the human brain is very good at
 it.
\end_layout

\begin_layout Standard
This means speech recognizers have to know which words are likely to come
 next and which are not.
 Fortunately, words can be predicted quite well without full understanding.
\end_layout

\begin_layout Section*
Literature survey
\end_layout

\begin_layout Subsection*
Standard Trigram method
\end_layout

\begin_layout Standard
Standard trigram method consists of taking a huge amount of text and counting
 the frequencies of all triples of words.
 Then using those frequencies to make bets on the relative probabilities
 of words, given the two previous words.
 Then we can compute the relative probability that the third word will be
 c (given first two words a, b), versus the thir that would be d, is given
 by the ratios of the counts:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{p(w_{3}=c|w_{2}=b,w_{1}=a)}{p(w_{3}=d|w_{2}=b,w_{1}=a)}=\frac{count(abc)}{count(abd)}
\]

\end_inset


\end_layout

\begin_layout Standard
Until recently, this was the state of the art.
\end_layout

\begin_layout Standard
The problem is that we cannot use a bigger context, because there are too
 many possibilities to store and the counts would mostly be zero.
\end_layout

\begin_layout Standard
Moreover, the probability of an arbitrary sequence is not zero just because
 we haven't seen that example triple before!
\end_layout

\begin_layout Standard
Moreover, the trigram model does not understands similarity between words,
 for example consider the sentence 
\begin_inset Quotes eld
\end_inset

the cat got squashed in the garden on friday
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

the dog got flattened in the yard on monday
\begin_inset Quotes erd
\end_inset

; so it cannot use past experience to help with new similar -but previously
 never seen- sequences.
\end_layout

\begin_layout Subsection*
Yoshua Bengio's Neural Probabilistic Model
\end_layout

\begin_layout Standard
To overcome this limitation, what we need to do is convert words into a
 vector of semantic and syntactic features, and use the features of the
 previous words to predict the next word.
\end_layout

\begin_layout Standard
Using a feature representation also allows a context that contains many
 more previous words (e.g.
 10).
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fig/bengio1.eps
	width 11cm

\end_inset


\end_layout

\begin_layout Standard
The problem with this architecture is that in the output, we have a very
 high (i.e.
 100000) number of outgoing connections, but we cannot afford to have a
 very high number of hidden units.
\end_layout

\begin_layout Subsection*
A Serial Architecture
\end_layout

\begin_layout Standard
One way to avoid is to slightly modify the structure as follows:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fig/bengio2.eps
	width 11cm

\end_inset


\end_layout

\begin_layout Standard
where we put in the input layers, not only the previous words, but also
 the candidate next word, and what we output is a score for how good is
 that word in that context.
\end_layout

\begin_layout Standard
(we have to run forward this net many many times)
\end_layout

\begin_layout Standard
So we try all the candidate words one at a time.
\end_layout

\begin_layout Standard
This allows the learned feature vector representation to be used for the
 candidate word.
\end_layout

\begin_layout Standard
After computing the logit score for each candidate word, use all of the
 logits in a softmax to get word probabilities
\end_layout

\begin_layout Part*
Neural network design
\end_layout

\begin_layout Standard
The network is composed by an input layer, whose size is N times the number
 of words in the dictionary; two hidden layers; and an output layer.
\end_layout

\begin_layout Standard
All units are logistic units with a sigmoid activation function 
\begin_inset Formula $\frac{1}{1+e^{-kx}}$
\end_inset

.
\end_layout

\begin_layout Standard
The output layer it is just one unit, and represent the logit of that word
 N-gram.
 High output (
\begin_inset Formula $\thickapprox1$
\end_inset

) means a N-gram is very likely.
 Low output (
\begin_inset Formula $\thickapprox-1$
\end_inset

) means a N-gram is very unlikely.
\end_layout

\begin_layout Standard
In the predictin task, the output is computed for every candidate word,
 and all the logits are transformed according to a softmax activation function:
\begin_inset Formula 
\[
f(x_{i})=\frac{e^{x_{i}}}{\sum_{j}e^{x_{j}}}
\]

\end_inset


\end_layout

\begin_layout Standard
so that we get a probability distribution of all the candidate words.
\end_layout

\begin_layout Standard
The cost function that has been used is the standard Mean Square Error function
 of FANN.
\end_layout

\begin_layout Section*
Dataset
\end_layout

\begin_layout Standard
One of the most challenging problems of this project was that of choosing
 a suitable dataset in order to obtain a good prediction.
 The size of the dataset used for the training and validation procedure,
 in fact, is a highly influencing aspect of this work: in detail, even if
 a big dataset would lead to better learning results, due to the distributed
 encoding which has been used in this implementation, the training procedure
 would require a lot of time and computational resources.
 Moreover, the team had to deal with a trade-off between the need for a
 big dataset and the need for a small dictionary in order to reduce the
 network size and the input vector
\end_layout

\begin_layout Subsection*
Dataset source
\end_layout

\begin_layout Standard
In order to obtain and produce a good quantity of text, suitable for the
 neural network training, validation and test, the team downloaded and pre
 processed many free e-books provided by the 
\begin_inset Quotes eld
\end_inset

Project Gutenberg
\begin_inset Quotes erd
\end_inset

.
 In detail, we produced two different datasets, splitting each of them 
\end_layout

\begin_layout Standard
*dictionary dictionary size
\end_layout

\begin_layout Standard
& encoding 1ofN
\end_layout

\begin_layout Standard
*N-gram model
\end_layout

\begin_layout Standard
*Negative examples generation
\end_layout

\begin_layout Section*
Neural network structure
\end_layout

\begin_layout Standard
repeat blah blah about structure
\end_layout

\begin_layout Subsection*
Local & distributed encoding of words
\end_layout

\begin_layout Standard
The network is 
\end_layout

\begin_layout Section*
Training Algorithm
\end_layout

\begin_layout Standard
blah!
\end_layout

\begin_layout Section*
Test Procedure
\end_layout

\begin_layout Subsection*
Bias and variance issues
\end_layout

\begin_layout Standard
blah blah overfitting
\end_layout

\begin_layout Section*
Results
\end_layout

\begin_layout Part*
Conclusions and Further Work
\end_layout

\begin_layout Standard
blah blah about possible RNN
\end_layout

\begin_layout Part*
References
\end_layout

\end_body
\end_document
